{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Recognition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM550giXkB5-"
      },
      "source": [
        "#Following command is to unzip the uploaded zip files \n",
        "#!unzip \"/content/speech-emotion-recognition-ravdess-data.zip\" -d \"/content/data/\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSbsN8I9XLDJ",
        "outputId": "76d6edce-a973-495a-aab6-e7355c270214"
      },
      "source": [
        "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libasound2-dev is already the newest version (1.1.3-5ubuntu0.6).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "Suggested packages:\n",
            "  portaudio19-doc\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0 portaudio19-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 184 kB of archives.\n",
            "After this operation, 891 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudio2 amd64 19.6.0-1 [64.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudiocpp0 amd64 19.6.0-1 [15.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 portaudio19-dev amd64 19.6.0-1 [104 kB]\n",
            "Fetched 184 kB in 1s (202 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1) ...\n",
            "Selecting previously unselected package libportaudiocpp0:amd64.\n",
            "Preparing to unpack .../libportaudiocpp0_19.6.0-1_amd64.deb ...\n",
            "Unpacking libportaudiocpp0:amd64 (19.6.0-1) ...\n",
            "Selecting previously unselected package portaudio19-dev:amd64.\n",
            "Preparing to unpack .../portaudio19-dev_19.6.0-1_amd64.deb ...\n",
            "Unpacking portaudio19-dev:amd64 (19.6.0-1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1) ...\n",
            "Setting up libportaudiocpp0:amd64 (19.6.0-1) ...\n",
            "Setting up portaudio19-dev:amd64 (19.6.0-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiiLqQoDXuIM",
        "outputId": "35fd5300-8814-48ad-f680-e152804fb677"
      },
      "source": [
        "pip install pyaudio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyaudio\n",
            "  Downloading PyAudio-0.2.11.tar.gz (37 kB)\n",
            "Building wheels for collected packages: pyaudio\n",
            "  Building wheel for pyaudio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyaudio: filename=PyAudio-0.2.11-cp37-cp37m-linux_x86_64.whl size=52614 sha256=579ef43308ff9c325e6c1d3d8a280d3e3b642e9f7f52e130fdfa4eeeb136977c\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/2e/4c/b71e7e96c861a46e6213bc6bb482b94dcf293a92c5e736c1ec\n",
            "Successfully built pyaudio\n",
            "Installing collected packages: pyaudio\n",
            "Successfully installed pyaudio-0.2.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpyic7OtrxlO"
      },
      "source": [
        "import librosa as li\n",
        "import numpy as np\n",
        "import soundfile as so\n",
        "import sklearn as sk\n",
        "import pyaudio as py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwEA8U_pX5U2"
      },
      "source": [
        "import librosa\n",
        "import soundfile\n",
        "import os, glob, pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLPOEM8DZ9Ag"
      },
      "source": [
        "#Extract features (mfcc, chroma, mel) from a sound file\n",
        "def extract_feature(file_name, mfcc, chroma, mel):\n",
        "    with soundfile.SoundFile(file_name) as sound_file:\n",
        "        X = sound_file.read(dtype=\"float32\")\n",
        "        sample_rate=sound_file.samplerate\n",
        "        if chroma:\n",
        "            stft=np.abs(librosa.stft(X))\n",
        "        result=np.array([])\n",
        "        if mfcc:\n",
        "            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
        "            result=np.hstack((result, mfccs))\n",
        "        if chroma:\n",
        "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
        "            result=np.hstack((result, chroma))\n",
        "        if mel:\n",
        "            mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
        "            result=np.hstack((result, mel))\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1XhuUyzbY8a"
      },
      "source": [
        "emotions={\n",
        "  '01':'neutral',\n",
        "  '02':'calm',\n",
        "  '03':'happy',\n",
        "  '04':'sad',\n",
        "  '05':'angry',\n",
        "  '06':'fearful',\n",
        "  '07':'disgust',\n",
        "  '08':'surprised'\n",
        "}\n",
        "\n",
        "#Emotions to observe\n",
        "observed_emotions=['calm', 'happy','angry']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCFA0D0bcEz_"
      },
      "source": [
        "def load_data(test_size=0.2):\n",
        "    x,y=[],[]\n",
        "    for file in glob.glob(\"data/Actor_*/*.wav\"):\n",
        "        file_name=os.path.basename(file)\n",
        "        emotion=emotions[file_name.split(\"-\")[2]]\n",
        "        if emotion not in observed_emotions:\n",
        "           continue\n",
        "        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
        "        x.append(feature)\n",
        "        y.append(emotion)\n",
        "    \n",
        "    x_norm = (x - np.mean(x, axis = 0)) / np.std(x, axis = 0)\n",
        "    return train_test_split( np.array(x_norm), y, test_size=test_size, random_state=9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ubceuih_ee2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58c8a3a9-56a1-4582-89a0-8d3f8f4c9d70"
      },
      "source": [
        "# load RAVDESS dataset\n",
        "X_train, X_test, y_train, y_test = load_data()\n",
        "\n",
        "# print some details_\n",
        "print(X_train)\n",
        "print(y_train)\n",
        "\n",
        "# number of samples in training data\n",
        "print(\"[+] Number of training samples:\", X_train.shape[0])\n",
        "# number of samples in testing data\n",
        "print(\"[+] Number of testing samples:\", X_test.shape[0])\n",
        "\n",
        "# number of features used\n",
        "# this is a vector of features extracted \n",
        "print(\"[+] Number of features:\", X_train.shape[1])\n",
        "\n",
        "model_params = {\n",
        "    'alpha': 0.05,\n",
        "    'batch_size': 256,\n",
        "    'epsilon': 1e-08, \n",
        "    'hidden_layer_sizes': (400,), \n",
        "    'learning_rate': 'adaptive', \n",
        "    'max_iter': 500, \n",
        "}\n",
        "# initialize Multi Layer Perceptron classifier\n",
        "model = MLPClassifier(**model_params)\n",
        "model_2 = SVC(gamma='scale',max_iter=500, verbose=0,kernel='rbf')\n",
        "\n",
        "# train the model\n",
        "print(\"[*] Training the model...\")\n",
        "model.fit(X_train, y_train)\n",
        "model_2.fit(X_train,y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y2_pred = model_2.predict(X_test)\n",
        "\n",
        "# calculate the accuracy\n",
        "accuracy_test = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
        "accuracy2_test = accuracy_score(y_true=y_test, y_pred=y2_pred)\n",
        "\n",
        "print(\"MLP_Accuracy_test: {:.2f}%\".format(accuracy_test*100))\n",
        "print(\"SVC_Accuracy_test: {:.2f}%\".format(accuracy2_test*100))\n",
        "\n",
        "# save the model\n",
        "if not os.path.isdir(\"result\"):\n",
        "    os.mkdir(\"result\")\n",
        "\n",
        "pickle.dump(model, open(\"result/mlp_classifier.model\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.53231106  0.97649648  2.05530561 ... -0.43933922 -0.38812868\n",
            "  -0.38885449]\n",
            " [-0.27464601  1.61600612  0.23269378 ... -0.38712241 -0.34444288\n",
            "  -0.36122703]\n",
            " [ 0.26461732  0.1635458   0.97922874 ... -0.19683084 -0.22209977\n",
            "  -0.27699305]\n",
            " ...\n",
            " [ 0.89978093 -1.04204285 -1.15820982 ... -0.34175526 -0.32875941\n",
            "  -0.32007317]\n",
            " [-0.02812381 -0.81587709 -0.32133688 ... -0.3060116  -0.24523744\n",
            "  -0.18378449]\n",
            " [-1.17618573  0.04399322  0.50810288 ... -0.41218631 -0.36992459\n",
            "  -0.36997293]]\n",
            "['calm', 'calm', 'angry', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'calm', 'angry', 'happy', 'calm', 'calm', 'happy', 'happy', 'happy', 'happy', 'calm', 'angry', 'calm', 'happy', 'happy', 'calm', 'angry', 'calm', 'happy', 'calm', 'angry', 'calm', 'happy', 'happy', 'happy', 'calm', 'calm', 'happy', 'angry', 'happy', 'happy', 'happy', 'happy', 'happy', 'angry', 'angry', 'angry', 'angry', 'angry', 'happy', 'angry', 'happy', 'happy', 'calm', 'angry', 'happy', 'angry', 'happy', 'calm', 'angry', 'happy', 'happy', 'calm', 'angry', 'angry', 'angry', 'angry', 'calm', 'happy', 'happy', 'angry', 'angry', 'happy', 'calm', 'calm', 'happy', 'calm', 'angry', 'calm', 'angry', 'calm', 'angry', 'calm', 'angry', 'happy', 'calm', 'angry', 'calm', 'angry', 'happy', 'calm', 'calm', 'happy', 'calm', 'calm', 'calm', 'happy', 'calm', 'happy', 'angry', 'calm', 'happy', 'happy', 'angry', 'angry', 'calm', 'happy', 'happy', 'calm', 'calm', 'angry', 'angry', 'calm', 'angry', 'calm', 'happy', 'calm', 'calm', 'happy', 'happy', 'angry', 'angry', 'happy', 'angry', 'calm', 'happy', 'happy', 'calm', 'happy', 'calm', 'calm', 'angry', 'happy', 'calm', 'calm', 'happy', 'angry', 'angry', 'angry', 'angry', 'calm', 'happy', 'calm', 'happy', 'angry', 'happy', 'happy', 'happy', 'angry', 'calm', 'calm', 'angry', 'happy', 'happy', 'angry', 'calm', 'happy', 'calm', 'angry', 'calm', 'happy', 'calm', 'calm', 'happy', 'angry', 'happy', 'calm', 'angry', 'happy', 'angry', 'happy', 'calm', 'angry', 'angry', 'angry', 'calm', 'happy', 'calm', 'happy', 'happy', 'angry', 'happy', 'angry', 'calm', 'calm', 'happy', 'calm', 'angry', 'happy', 'angry', 'angry', 'calm', 'happy', 'calm', 'happy', 'calm', 'calm', 'calm', 'angry', 'happy', 'happy', 'happy', 'angry', 'calm', 'angry', 'angry', 'angry', 'calm', 'happy', 'calm', 'happy', 'calm', 'happy', 'happy', 'angry', 'happy', 'calm', 'calm', 'happy', 'happy', 'calm', 'angry', 'calm', 'angry', 'angry', 'calm', 'calm', 'angry', 'calm', 'angry', 'happy', 'calm', 'happy', 'happy', 'happy', 'calm', 'calm', 'happy', 'calm', 'happy', 'calm', 'happy', 'calm', 'angry', 'angry', 'happy', 'happy', 'calm', 'angry', 'angry', 'happy', 'calm', 'angry', 'angry', 'angry', 'angry', 'happy', 'calm', 'happy', 'calm', 'happy', 'angry', 'calm', 'angry', 'calm', 'angry', 'happy', 'calm', 'calm', 'happy', 'angry', 'happy', 'calm', 'happy', 'happy', 'angry', 'happy', 'calm', 'calm', 'calm', 'happy', 'happy', 'angry', 'calm', 'angry', 'happy', 'angry', 'angry', 'calm', 'angry', 'happy', 'calm', 'calm', 'calm', 'happy', 'calm', 'angry', 'happy', 'angry', 'happy', 'angry', 'happy', 'angry', 'calm', 'happy', 'angry', 'angry', 'happy', 'happy', 'angry', 'calm', 'calm', 'happy', 'happy', 'angry', 'calm', 'angry', 'calm', 'happy', 'calm', 'calm', 'calm', 'angry', 'angry', 'calm', 'calm', 'calm', 'angry', 'angry', 'calm', 'calm', 'happy', 'happy', 'calm', 'angry', 'angry', 'happy', 'happy', 'angry', 'angry', 'happy', 'calm', 'happy', 'happy', 'angry', 'happy', 'calm', 'angry', 'calm', 'happy', 'happy', 'happy', 'calm', 'angry', 'happy', 'angry', 'calm', 'angry', 'angry', 'calm', 'calm', 'happy', 'angry', 'calm', 'happy', 'calm', 'happy', 'calm', 'happy', 'angry', 'happy', 'happy', 'happy', 'happy', 'angry', 'angry', 'calm', 'angry', 'angry', 'happy', 'angry', 'happy', 'calm', 'happy', 'happy', 'calm', 'angry', 'calm', 'calm', 'happy', 'angry', 'calm', 'happy', 'angry', 'calm', 'happy', 'happy', 'happy', 'calm', 'angry', 'happy', 'happy', 'happy', 'happy', 'angry', 'angry', 'calm', 'angry', 'happy', 'happy', 'happy', 'calm', 'happy', 'angry', 'calm', 'calm', 'happy', 'angry', 'angry', 'calm', 'angry', 'happy', 'happy', 'calm', 'calm', 'angry', 'happy', 'happy', 'angry', 'happy', 'calm', 'happy', 'angry', 'angry', 'calm', 'angry', 'angry', 'angry', 'calm', 'calm', 'calm', 'happy', 'calm', 'calm', 'happy', 'angry', 'calm', 'calm', 'angry', 'angry', 'happy', 'calm', 'angry', 'calm', 'angry', 'happy', 'angry', 'happy', 'happy', 'angry', 'angry', 'calm']\n",
            "[+] Number of training samples: 460\n",
            "[+] Number of testing samples: 116\n",
            "[+] Number of features: 180\n",
            "[*] Training the model...\n",
            "MLP_Accuracy_test: 85.34%\n",
            "SVC_Accuracy_test: 68.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "izKaSrv7LLiC",
        "outputId": "4c5b9d0a-856e-4d7e-d87e-59f61cdb2284"
      },
      "source": [
        "data = {'MLP_Accuracy_test':accuracy_test, 'SVC_Accuracy_test':accuracy2_test}\n",
        "Classifiers = list(data.keys())\n",
        "values = list(data.values())\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "\n",
        "plt.bar(Classifiers, values, color=['green','blue'],width=0.4)\n",
        "\n",
        "plt.xlabel(\"Classifiers\")\n",
        "plt.ylabel(\"Accuracy Obtained\")\n",
        "plt.title(\"Comparision of Classifiers\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-59c41064ba19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'MLP_Accuracy_test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0maccuracy_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SVC_Accuracy_test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0maccuracy2_test\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mClassifiers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'accuracy_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.cuda as cuda\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "# Set proxy in case it's not already set in our environment before loading torchvision\n",
        "\n",
        "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
        "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
        "\n",
        "import torchvision\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "# The functional module contains helper functions for defining neural network layers as simple functions\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "ikvR6iPENdVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = 0.0\n",
        "stddev = 1.0\n",
        "\n",
        "# Transform to apply to input images\n",
        "\n",
        "\n",
        "# Datasets\n",
        "x_train = torch.tensor(X_train.max(axis=0))\n",
        "x_test = torch.tensor(X_test.max(axis=0))\n",
        "\n",
        "batch_size = 1024\n",
        "X_train_loader = torch.utils.data.DataLoader(x_train, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "X_valid_loader = torch.utils.data.DataLoader(x_test, batch_size=batch_size, shuffle=True, num_workers=1)\n"
      ],
      "metadata": {
        "id": "R8qi7AnMNkof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SafyeH-EQ6M7"
      },
      "source": [
        "# 1D CNN Approach \n",
        "class CNN_Model(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "               \n",
        "        # NOTE: All Conv2d layers have a default padding of 0 and stride of 1,\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)      # 24 x 24 x 20  (after 1st convolution)\n",
        "        self.relu1 = nn.ReLU()                            # Same as above\n",
        "        \n",
        "        # Convolution Layer 2\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)     # 20 x 20 x 20  (after 2nd convolution)\n",
        "        #self.conv2_drop = nn.Dropout2d(p=0.5)            # Dropout is a regularization technqiue we discussed in class\n",
        "        self.maxpool2 = nn.MaxPool2d(2)                   # 10 x 10 x 20  (after pooling)\n",
        "        self.relu2 = nn.ReLU()                            # Same as above \n",
        "        \n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(2000, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # Convolution Layer 1                    \n",
        "        x = self.conv1(x)                        \n",
        "        x = self.relu1(x)                        \n",
        "        \n",
        "        # Convolution Layer 2\n",
        "        x = self.conv2(x)               \n",
        "        #x = self.conv2_drop(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.relu2(x)\n",
        "        \n",
        "        # Switch from activation maps to vectors\n",
        "        x = x.view(-1, 2000)\n",
        "        \n",
        "        # Fully connected layer 1\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        #x = F.dropout(x, training=True)\n",
        "        \n",
        "        # Fully connected layer 2\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The model\n",
        "net = CNN_Model()\n",
        "\n",
        "device = 'cuda:2'#change the GPU if you want from 0 to 4, also substitute the device eveywhere you see cuda(),to see the process in terminal nvdia-smi\n",
        "if cuda.is_available():\n",
        "    net = net.cuda(device)\n",
        "\n",
        "# Our loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Our optimizer\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9) "
      ],
      "metadata": {
        "id": "DTdqlCQYNB5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "train_accuracy = []\n",
        "valid_accuracy = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    ############################\n",
        "    # Train\n",
        "    ############################\n",
        "    \n",
        "    iter_loss = 0.0\n",
        "    correct = 0\n",
        "    iterations = 0\n",
        "    \n",
        "    net.train()                   # Put the network into training mode\n",
        "    \n",
        "    for i, (items, classes) in enumerate(X_train_loader):\n",
        "        \n",
        "        # Convert torch tensor to Variable\n",
        "        items = Variable(items)\n",
        "        classes = Variable(classes)\n",
        "        \n",
        "        # If we have GPU, shift the data to GPU\n",
        "        if cuda.is_available():\n",
        "            items = items.cuda(device)\n",
        "            classes = classes.cuda(device)\n",
        "        \n",
        "        optimizer.zero_grad()     # Clear off the gradients from any past operation\n",
        "        outputs = net(items)      # Do the forward pass\n",
        "        loss = criterion(outputs, classes) # Calculate the loss\n",
        "        iter_loss += loss.item() # Accumulate the loss\n",
        "        loss.backward()           # Calculate the gradients with help of back propagation\n",
        "        optimizer.step()          # Ask the optimizer to adjust the parameters based on the gradients\n",
        "        \n",
        "        # Record the correct predictions for training data \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += (predicted == classes.data).sum()\n",
        "        iterations += 1\n",
        "    \n",
        "    # Record the training loss\n",
        "    train_loss.append(iter_loss / iterations)\n",
        "    # Record the training accuracy\n",
        "    train_accuracy.append((100 * correct.item() / float(len(X_train_loader.dataset))))\n",
        "   \n",
        "\n",
        "    ############################\n",
        "    # Validate - How did we do on the unseen dataset?\n",
        "    ############################\n",
        "    \n",
        "    loss = 0.0\n",
        "    correct = 0\n",
        "    iterations = 0\n",
        "\n",
        "    net.eval()                    # Put the network into evaluate mode\n",
        "    \n",
        "    for i, (items, classes) in enumerate(X_valid_loader):\n",
        "        \n",
        "        # Convert torch tensor to Variable\n",
        "        items = Variable(items)\n",
        "        classes = Variable(classes)\n",
        "        \n",
        "        # If we have GPU, shift the data to GPU\n",
        "        if cuda.is_available():\n",
        "            items = items.cuda(device)\n",
        "            classes = classes.cuda(device)\n",
        "        \n",
        "        outputs = net(items)      # Do the forward pass\n",
        "        loss += criterion(outputs, classes).item() # Calculate the loss\n",
        "        \n",
        "        # Record the correct predictions for training data\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += (predicted == classes.data).sum()\n",
        "        \n",
        "        iterations += 1\n",
        "\n",
        "    # Record the validation loss\n",
        "    valid_loss.append(loss / iterations)\n",
        "    # Record the validation accuracy\n",
        "    correct_scalar = np.array([correct.clone().cpu()])[0]\n",
        "    valid_accuracy.append(correct_scalar.item() / len(X_valid_loader.dataset) * 100.0)\n",
        "\n",
        "    print ('Epoch %d/%d, Tr Loss: %.4f, Tr Acc: %.4f, Val Loss: %.4f, Val Acc: %.4f'\n",
        "           %(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], \n",
        "             valid_loss[-1], valid_accuracy[-1]))\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "FqF5ZZRFNGNb",
        "outputId": "47950df2-f700-459a-8835-e288d1241598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-136-f3904d0bc9bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                   \u001b[0;31m# Put the network into training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Convert torch tensor to Variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = plt.figure(figsize=(10, 8))\n",
        "plt.plot(train_loss, label='training loss')\n",
        "plt.plot(valid_loss, label='validation loss')\n",
        "plt.title('MNIST loss, conv-5x5x10-relu-conv-5x5x20-maxpool-2x2-relu-fc50-fc10')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZZuKrfXcNKkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "f = plt.figure(figsize=(10, 8))\n",
        "plt.plot(train_accuracy, label='training accuracy')\n",
        "plt.plot(valid_accuracy, label='validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qEiOhNhJNOzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXMO327pryLR"
      },
      "source": [
        ""
      ]
    }
  ]
}